

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">

  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/background/%E5%9B%BE%E6%A0%87.png">
  

  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="文晋">
  <meta name="keywords" content="">
  
    <meta name="description" content=".tboncnknkoym{}   参考项目：naklecha&#x2F;llama3-from-scratch、图解llama架构 解读源码实现 完整结构：Llama 内部结构拆解 1 准备工作1.1 创建虚拟环境12345conda create -n llama3 python&#x3D;3.10conda activate llama3pip install sentencepiece t">
<meta property="og:type" content="article">
<meta property="og:title" content="【模型复现】从零实现 Llama3">
<meta property="og:url" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/index.html">
<meta property="og:site_name" content="文心晋意">
<meta property="og:description" content=".tboncnknkoym{}   参考项目：naklecha&#x2F;llama3-from-scratch、图解llama架构 解读源码实现 完整结构：Llama 内部结构拆解 1 准备工作1.1 创建虚拟环境12345conda create -n llama3 python&#x3D;3.10conda activate llama3pip install sentencepiece t">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/1.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/2.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/3.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/4.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/5.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/6.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/7.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/8.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/9.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/10.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/11.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/12.png">
<meta property="og:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/13.png">
<meta property="article:published_time" content="2025-04-01T04:00:00.000Z">
<meta property="article:modified_time" content="2025-04-26T10:05:29.629Z">
<meta property="article:author" content="文晋">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://xuan-van.github.io/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/1.png">
  
  
  
  <title>【模型复现】从零实现 Llama3 - 文心晋意</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1749284_5i9bdhy70f8.css">



<link rel="stylesheet" href="//at.alicdn.com/t/c/font_1736178_k526ubmyhba.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  



  
<link rel="stylesheet" href="/background/background.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"xuan-van.github.io","root":"/","version":"1.9.8","typing":{"enable":false,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":3},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false},"umami":{"src":null,"website_id":null,"domains":null,"start_time":"2024-01-01T00:00:00.000Z","token":null,"api_server":null}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.3.0"></head>


<body><!-- hexo injector body_begin start --><div id="web_bg"></div><!-- hexo injector body_begin end -->
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>文晋的博客</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" target="_self" href="javascript:;" role="button"
                 data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                <i class="iconfont icon-image"></i>
                <span>图片</span>
              </a>
              <div class="dropdown-menu" aria-labelledby="navbarDropdown">
                
                  
                  
                  
                  <a class="dropdown-item" href="/images/llama.svg" target="_self">
                    
                    <span>Llama 结构</span>
                  </a>
                
              </div>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/default.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle">【模型复现】从零实现 Llama3</span>
          
        </div>

        
          
  <div class="mt-3">
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2025-04-01 12:00" pubdate>
          2025年4月1日 中午
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.8k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          41 分钟
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar category-bar" style="margin-right: -1rem">
    





<div class="category-list">
  
  
    
    
    
    <div class="category row nomargin-x">
      <a class="category-item 
          list-group-item category-item-action col-10 col-md-11 col-xm-11" title="代码复现"
        id="heading-211acd8e7f189296b7caddd5c95b71af" role="tab" data-toggle="collapse" href="#collapse-211acd8e7f189296b7caddd5c95b71af"
        aria-expanded="true"
      >
        代码复现
        <span class="list-group-count">(7)</span>
        <i class="iconfont icon-arrowright"></i>
      </a>
      
      <div class="category-collapse collapse show" id="collapse-211acd8e7f189296b7caddd5c95b71af"
           role="tabpanel" aria-labelledby="heading-211acd8e7f189296b7caddd5c95b71af">
        
        
          
  <div class="category-post-list">
    
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E4%BB%A3%E7%A0%81%E6%8B%86%E8%A7%A3%E3%80%91trajectory-transformer/" title="【代码拆解】Trajectory Transformer"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【代码拆解】Trajectory Transformer</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87%E3%80%91%E5%B8%B8%E7%94%A8%E9%97%AE%E7%AD%94%E6%95%B0%E6%8D%AE%E9%9B%86/" title="【数据准备】常用问答数据集"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【数据准备】常用问答数据集</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/" title="【模型复现】从零实现 Llama3"
           class="list-group-item list-group-item-action
           active">
          <span class="category-post">【模型复现】从零实现 Llama3</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91hipporag-hipporag2/" title="【论文复现】HippoRAG &amp; HippoRAG2"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【论文复现】HippoRAG &amp; HippoRAG2</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91instructrag/" title="【论文复现】InstructRAG"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【论文复现】InstructRAG</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91selfrag/" title="【论文复现】SelfRAG"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【论文复现】SelfRAG</span>
        </a>
      
    
      
      
        <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91xrag/" title="【论文复现】xRAG"
           class="list-group-item list-group-item-action
           ">
          <span class="category-post">【论文复现】xRAG</span>
        </a>
      
    
  </div>

        
      </div>
    </div>
  
</div>


  </aside>


    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">【模型复现】从零实现 Llama3</h1>
            
            
              <div class="markdown-body">
                
                <figure style="text-align: center;">
    <style>.tboncnknkoym{}</style><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/1.png" srcset="/img/loading.gif" lazyload class="tboncnknkoym">
</figure>

<p>参考项目：<a target="_blank" rel="noopener" href="https://github.com/naklecha/llama3-from-scratch">naklecha&#x2F;llama3-from-scratch</a>、<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1nK4y1F7x7/?share_source=copy_web&vd_source=050f6c168e089d7aca0de02effbc8434">图解llama架构 解读源码实现</a></p>
<p>完整结构：<a href="https://xuan-van.github.io/images/llama.svg">Llama 内部结构拆解</a></p>
<h1 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1 准备工作"></a>1 准备工作</h1><h2 id="1-1-创建虚拟环境"><a href="#1-1-创建虚拟环境" class="headerlink" title="1.1 创建虚拟环境"></a>1.1 创建虚拟环境</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">conda create -n llama3 python=<span class="hljs-number">3.10</span><br>conda activate llama3<br>pip install sentencepiece tiktoken torch blobfile matplotlib ipykernel<br>python -m ipykernel install --user --name llama3<br>jupyter kernelspec <span class="hljs-built_in">list</span><br></code></pre></td></tr></table></figure>

<h2 id="1-2-下载模型文件"><a href="#1-2-下载模型文件" class="headerlink" title="1.2 下载模型文件"></a>1.2 下载模型文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">huggingface-cli download meta-llama/Meta-Llama-3-8B --include <span class="hljs-string">&quot;original/*&quot;</span> --token Your_token --local-dir model/Llama-3-8B<br></code></pre></td></tr></table></figure>

<h2 id="1-3-创建分词器"><a href="#1-3-创建分词器" class="headerlink" title="1.3 创建分词器"></a>1.3 创建分词器</h2><p>使用 BPE 分词器，来自 <a target="_blank" rel="noopener" href="https://github.com/karpathy/minbpe">karpathy&#x2F;minbpe</a>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> pathlib <span class="hljs-keyword">import</span> Path<br><span class="hljs-keyword">import</span> tiktoken<br><span class="hljs-keyword">from</span> tiktoken.load <span class="hljs-keyword">import</span> load_tiktoken_bpe<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> json<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br><span class="hljs-comment"># 加载分词器模型路径</span><br>tokenizer_path = <span class="hljs-string">&quot;../model/Llama-3-8B/original/tokenizer.model&quot;</span><br>special_tokens = [<br>            <span class="hljs-string">&quot;&lt;|begin_of_text|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|end_of_text|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_0|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_1|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_2|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_3|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|start_header_id|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|end_header_id|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|reserved_special_token_4|&gt;&quot;</span>,<br>            <span class="hljs-string">&quot;&lt;|eot_id|&gt;&quot;</span>, <span class="hljs-comment"># 对话结束标记</span><br>        ] + [<span class="hljs-string">f&quot;&lt;|reserved_special_token_<span class="hljs-subst">&#123;i&#125;</span>|&gt;&quot;</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">5</span>, <span class="hljs-number">256</span> - <span class="hljs-number">5</span>)]<br>        <br><span class="hljs-comment"># 加载BPE（Byte Pair Encoding）分词器的合并表</span><br>mergeable_ranks = load_tiktoken_bpe(tokenizer_path)<br><span class="hljs-comment"># 创建tiktoken编码器实例</span><br>tokenizer = tiktoken.Encoding(<br>    name=Path(tokenizer_path).name, <span class="hljs-comment"># 使用文件名作为编码器名称</span><br>    pat_str=<span class="hljs-string">r&quot;(?i:&#x27;s|&#x27;t|&#x27;re|&#x27;ve|&#x27;m|&#x27;ll|&#x27;d)|[^\r\n\p&#123;L&#125;\p&#123;N&#125;]?\p&#123;L&#125;+|\p&#123;N&#125;&#123;1,3&#125;| ?[^\s\p&#123;L&#125;\p&#123;N&#125;]+[\r\n]*|\s*[\r\n]+|\s+(?!\S)|\s+&quot;</span>, <span class="hljs-comment"># 定义分词的正则表达式模式</span><br>    mergeable_ranks=mergeable_ranks, <span class="hljs-comment"># 设置BPE合并表</span><br>    special_tokens=&#123;token: <span class="hljs-built_in">len</span>(mergeable_ranks) + i <span class="hljs-keyword">for</span> i, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(special_tokens)&#125;, <span class="hljs-comment"># 设置特殊token及其对应的ID</span><br>)<br><br><span class="hljs-comment"># 测试分词器编码和解码功能</span><br>tokenizer.decode(tokenizer.encode(<span class="hljs-string">&quot;hello world!&quot;</span>))<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">&#39;hello world!&#39;
</code></pre>
<h2 id="1-4-加载模型参数"><a href="#1-4-加载模型参数" class="headerlink" title="1.4 加载模型参数"></a>1.4 加载模型参数</h2><ol>
<li>加载模型权重：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">model = torch.load(<span class="hljs-string">&quot;../model/Llama-3-8B/original/consolidated.00.pth&quot;</span>)<br><span class="hljs-built_in">print</span>(json.dumps(<span class="hljs-built_in">list</span>(model.keys())[:<span class="hljs-number">20</span>], indent=<span class="hljs-number">4</span>))<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">[
    &quot;tok_embeddings.weight&quot;,
    &quot;layers.0.attention.wq.weight&quot;,
    &quot;layers.0.attention.wk.weight&quot;,
    &quot;layers.0.attention.wv.weight&quot;,
    &quot;layers.0.attention.wo.weight&quot;,
    &quot;layers.0.feed_forward.w1.weight&quot;,
    &quot;layers.0.feed_forward.w3.weight&quot;,
    &quot;layers.0.feed_forward.w2.weight&quot;,
    &quot;layers.0.attention_norm.weight&quot;,
    &quot;layers.0.ffn_norm.weight&quot;,
    &quot;layers.1.attention.wq.weight&quot;,
    &quot;layers.1.attention.wk.weight&quot;,
    &quot;layers.1.attention.wv.weight&quot;,
    &quot;layers.1.attention.wo.weight&quot;,
    &quot;layers.1.feed_forward.w1.weight&quot;,
    &quot;layers.1.feed_forward.w3.weight&quot;,
    &quot;layers.1.feed_forward.w2.weight&quot;,
    &quot;layers.1.attention_norm.weight&quot;,
    &quot;layers.1.ffn_norm.weight&quot;,
    &quot;layers.2.attention.wq.weight&quot;
]
</code></pre>
<ol start="2">
<li>加载配置文件：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;../model/Llama-3-8B/original/params.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    config = json.load(f)<br>config <br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">&#123;&#39;dim&#39;: 4096,
 &#39;n_layers&#39;: 32,
 &#39;n_heads&#39;: 32,
 &#39;n_kv_heads&#39;: 8,
 &#39;vocab_size&#39;: 128256,
 &#39;multiple_of&#39;: 1024,
 &#39;ffn_dim_multiplier&#39;: 1.3,
 &#39;norm_eps&#39;: 1e-05,
 &#39;rope_theta&#39;: 500000.0&#125;
 
</code></pre>
<ol start="3">
<li>定义模型参数：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">dim = config[<span class="hljs-string">&quot;dim&quot;</span>] <span class="hljs-comment"># 模型的向量维度：每个token被编码为4096大小的向量</span><br>n_layers = config[<span class="hljs-string">&quot;n_layers&quot;</span>] <span class="hljs-comment"># Transformer层数/深度：模型有32个连续的Transformer块</span><br>n_heads = config[<span class="hljs-string">&quot;n_heads&quot;</span>] <span class="hljs-comment"># 注意力头数量：同时关注不同位置的不同表示子空间</span><br>n_kv_heads = config[<span class="hljs-string">&quot;n_kv_heads&quot;</span>] <span class="hljs-comment"># 用于key和value的注意力头数量：多个查询头会共享相同的key/value头</span><br>vocab_size = config[<span class="hljs-string">&quot;vocab_size&quot;</span>] <span class="hljs-comment"># 词汇表大小：模型能识别128256种不同的token</span><br>multiple_of = config[<span class="hljs-string">&quot;multiple_of&quot;</span>] <span class="hljs-comment"># MLP维度的对齐基数</span><br>ffn_dim_multiplier = config[<span class="hljs-string">&quot;ffn_dim_multiplier&quot;</span>] <span class="hljs-comment"># MLP维度的乘数：dim * multiple_of * ffn_dim_multiplier</span><br>norm_eps = config[<span class="hljs-string">&quot;norm_eps&quot;</span>] <span class="hljs-comment"># 层归一化（RMSNorm）中的epsilon值：用于数值稳定性的小常数，防止除零</span><br>rope_theta = torch.tensor(config[<span class="hljs-string">&quot;rope_theta&quot;</span>]) <span class="hljs-comment"># RoPE（Rotary Position Embedding）的位置编码基数：控制位置编码的频率缩放，较大的theta值可以扩展模型的上下文处理能力</span><br></code></pre></td></tr></table></figure>

<h1 id="2-分词"><a href="#2-分词" class="headerlink" title="2 分词"></a>2 分词</h1><ol>
<li>将输入的文本通过分词器变成 token ID：</li>
</ol>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/2.png" srcset="/img/loading.gif" lazyload class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">&quot;the answer to the ultimate question of life, the universe, and everything is &quot;</span><br><span class="hljs-comment"># 编码为token</span><br>tokens = [<span class="hljs-number">128000</span>] + tokenizer.encode(prompt)<br><span class="hljs-built_in">print</span>(tokens)<br>tokens = torch.tensor(tokens)<br><br><span class="hljs-comment"># 将每个 token 解码为对应的文本</span><br>prompt_split_as_tokens = [tokenizer.decode([token.item()]) <span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> tokens]<br><span class="hljs-built_in">print</span>(prompt_split_as_tokens)<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">[128000, 1820, 4320, 311, 279, 17139, 3488, 315, 2324, 11, 279, 15861, 11, 323, 4395, 374, 220]
[&#39;&lt;|begin_of_text|&gt;&#39;, &#39;the&#39;, &#39; answer&#39;, &#39; to&#39;, &#39; the&#39;, &#39; ultimate&#39;, &#39; question&#39;, &#39; of&#39;, &#39; life&#39;, &#39;,&#39;, &#39; the&#39;, &#39; universe&#39;, &#39;,&#39;, &#39; and&#39;, &#39; everything&#39;, &#39; is&#39;, &#39; &#39;]
</code></pre>
<ol start="2">
<li>将 17 个 token ID 转换成 17 个 token embedding：</li>
</ol>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/3.png" srcset="/img/loading.gif" lazyload class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">embedding_layer = torch.nn.Embedding(vocab_size, dim)<br>embedding_layer.weight.data.copy_(model[<span class="hljs-string">&quot;tok_embeddings.weight&quot;</span>])<br>token_embeddings_unnormalized = embedding_layer(tokens).to(torch.bfloat16)<br>token_embeddings_unnormalized.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<h1 id="3-RMSNorm"><a href="#3-RMSNorm" class="headerlink" title="3 RMSNorm"></a>3 RMSNorm</h1><p>这种归一化的方法可以在保持精度的情况下最大化计算效率，张量形状不变，过程如下：</p>
<ol>
<li>计算输入张量每个元素的平方。</li>
<li>对平方后的张量沿着最后一个维度计算均值，并保持维度不变，这样得到每个元素的均方值。</li>
<li>将均方值加上一个很小的正数（避免除以零），然后计算其平方根的倒数，得到 RMS 的倒数。</li>
<li>将输入张量与 RMS 的倒数相乘，再乘以归一化权重，得到归一化后的张量。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># def rms_norm(tensor, norm_weights):</span><br><span class="hljs-comment">#     rms = (tensor.pow(2).mean(-1, keepdim=True) + norm_eps)**0.5</span><br><span class="hljs-comment">#     return tensor * (norm_weights / rms)</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">rms_norm</span>(<span class="hljs-params">tensor, norm_weights</span>):<br>    <span class="hljs-keyword">return</span> (tensor * torch.rsqrt(tensor.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>) + norm_eps)) * norm_weights<br></code></pre></td></tr></table></figure>

<p>对 17 个 token embedding 归一化：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">token_embeddings = rms_norm(token_embeddings_unnormalized, model[<span class="hljs-string">&quot;layers.0.attention_norm.weight&quot;</span>])<br>token_embeddings.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<h1 id="4-注意力头：以第一层为例"><a href="#4-注意力头：以第一层为例" class="headerlink" title="4 注意力头：以第一层为例"></a>4 注意力头：以第一层为例</h1><p>查看第一层所有注意力头的权重矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(<br>    model[<span class="hljs-string">&quot;layers.0.attention.wq.weight&quot;</span>].shape, <span class="hljs-comment"># query</span><br>    model[<span class="hljs-string">&quot;layers.0.attention.wk.weight&quot;</span>].shape, <span class="hljs-comment"># key</span><br>    model[<span class="hljs-string">&quot;layers.0.attention.wv.weight&quot;</span>].shape, <span class="hljs-comment"># value</span><br>    model[<span class="hljs-string">&quot;layers.0.attention.wo.weight&quot;</span>].shape  <span class="hljs-comment"># output</span><br>)<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([4096, 4096]) torch.Size([1024, 4096]) torch.Size([1024, 4096]) torch.Size([4096, 4096])
</code></pre>
<h2 id="4-1-单头注意力：以第一层的第一个注意力头为例"><a href="#4-1-单头注意力：以第一层的第一个注意力头为例" class="headerlink" title="4.1 单头注意力：以第一层的第一个注意力头为例"></a>4.1 单头注意力：以第一层的第一个注意力头为例</h2><h3 id="4-1-1-query"><a href="#4-1-1-query" class="headerlink" title="4.1.1 query"></a>4.1.1 query</h3><ol>
<li>查看第一层所有注意力头的 query 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">q_layer0 = model[<span class="hljs-string">&quot;layers.0.attention.wq.weight&quot;</span>]<br>head_dim = q_layer0.shape[<span class="hljs-number">0</span>] // n_heads<br>q_layer0 = q_layer0.view(n_heads, head_dim, dim)<br>q_layer0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([32, 128, 4096])
</code></pre>
<ol start="2">
<li>查看第一层第一个注意力头的 query 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_layer0_head0 = q_layer0[<span class="hljs-number">0</span>]<br>q_layer0_head0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([128, 4096])
</code></pre>
<ol start="3">
<li>将第一层第一个注意力头的 query 权重矩阵与 token embedding 相乘，得到每个 token 的 query：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token = torch.matmul(token_embeddings, q_layer0_head0.T)<br>q_per_token.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h3 id="4-1-2-key"><a href="#4-1-2-key" class="headerlink" title="4.1.2 key"></a>4.1.2 key</h3><p>key 和 query 的计算流程一致，不过其权重矩阵只有 query 权重矩阵的 1&#x2F;4，因为 key 的权重矩阵在 4 个头之间共享，以减少所需的计算量。</p>
<blockquote>
<p>Grouped Multi-Query Attention（分组多查询注意力）是一种 平衡计算效率和模型性能 的注意力变体，介于 Multi-Head Attention (MHA) 和 Multi-Query Attention (MQA) 之间。它通过分组共享 Key&#x2F;Value 矩阵 来减少计算量，同时保持较强的表达能力。</p>
</blockquote>
<ol>
<li>查看第一层所有注意力头的 key 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">k_layer0 = model[<span class="hljs-string">&quot;layers.0.attention.wk.weight&quot;</span>]<br>k_layer0 = k_layer0.view(n_kv_heads, k_layer0.shape[<span class="hljs-number">0</span>] // n_kv_heads, dim)<br>k_layer0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([8, 128, 4096])
</code></pre>
<ol start="2">
<li>查看第一层第一个注意力头的 key 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_layer0_head0 = k_layer0[<span class="hljs-number">0</span>]<br>k_layer0_head0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([128, 4096])
</code></pre>
<ol start="3">
<li>将第一层第一个注意力头的 key 的权重矩阵与 token embedding 相乘，得到每个 token 的 key：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_per_token = torch.matmul(token_embeddings, k_layer0_head0.T)<br>k_per_token.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h3 id="4-1-3-value"><a href="#4-1-3-value" class="headerlink" title="4.1.3 value"></a>4.1.3 value</h3><p>和 key 一样，value 权重矩阵也在每 4 个注意力头之间进行共享。</p>
<blockquote>
<p>KV Cache 用于加速自回归生成，其核心思想是缓存已计算的 key 和 value 矩阵，避免重复计算，从而显著减少推理时的计算量和内存访问开销。</p>
</blockquote>
<ol>
<li>查看第一层所有注意力头的 value 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">v_layer0 = model[<span class="hljs-string">&quot;layers.0.attention.wv.weight&quot;</span>]<br>v_layer0 = v_layer0.view(n_kv_heads, v_layer0.shape[<span class="hljs-number">0</span>] // n_kv_heads, dim)<br>v_layer0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([8, 128, 4096])
</code></pre>
<ol start="2">
<li>查看第一层第一个注意力头的 value 的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">v_layer0_head0 = v_layer0[<span class="hljs-number">0</span>]<br>v_layer0_head0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([128, 4096])
</code></pre>
<ol start="3">
<li>将第一层第一个注意力头的 value 的权重矩阵与 token embedding 相乘，得到每个 token 的 value：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">v_per_token = torch.matmul(token_embeddings, v_layer0_head0.T)<br>v_per_token.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h3 id="4-1-4-RoPE（旋转位置编码）"><a href="#4-1-4-RoPE（旋转位置编码）" class="headerlink" title="4.1.4 RoPE（旋转位置编码）"></a>4.1.4 RoPE（旋转位置编码）</h3><p>每个 token 都有一个 query 和 key，但是每个 query 和 key 并不知道它们在文本中的位置，因此需要进行位置编码。</p>
<ol>
<li>创建一个包含 64 个元素的张量：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">zero_to_one_split_into_64_parts = torch.tensor(<span class="hljs-built_in">range</span>(<span class="hljs-number">64</span>))/<span class="hljs-number">64</span><br>zero_to_one_split_into_64_parts<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">tensor([0.0000, 0.0156, 0.0312, 0.0469, 0.0625, 0.0781, 0.0938, 0.1094, 0.1250,
        0.1406, 0.1562, 0.1719, 0.1875, 0.2031, 0.2188, 0.2344, 0.2500, 0.2656,
        0.2812, 0.2969, 0.3125, 0.3281, 0.3438, 0.3594, 0.3750, 0.3906, 0.4062,
        0.4219, 0.4375, 0.4531, 0.4688, 0.4844, 0.5000, 0.5156, 0.5312, 0.5469,
        0.5625, 0.5781, 0.5938, 0.6094, 0.6250, 0.6406, 0.6562, 0.6719, 0.6875,
        0.7031, 0.7188, 0.7344, 0.7500, 0.7656, 0.7812, 0.7969, 0.8125, 0.8281,
        0.8438, 0.8594, 0.8750, 0.8906, 0.9062, 0.9219, 0.9375, 0.9531, 0.9688,
        0.9844])
</code></pre>
<ol start="2">
<li>计算频率值：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">freqs = <span class="hljs-number">1.0</span> / (rope_theta ** zero_to_one_split_into_64_parts)<br>freqs<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">tensor([1.0000e+00, 8.1462e-01, 6.6360e-01, 5.4058e-01, 4.4037e-01, 3.5873e-01,
        2.9223e-01, 2.3805e-01, 1.9392e-01, 1.5797e-01, 1.2869e-01, 1.0483e-01,
        8.5397e-02, 6.9566e-02, 5.6670e-02, 4.6164e-02, 3.7606e-02, 3.0635e-02,
        2.4955e-02, 2.0329e-02, 1.6560e-02, 1.3490e-02, 1.0990e-02, 8.9523e-03,
        7.2927e-03, 5.9407e-03, 4.8394e-03, 3.9423e-03, 3.2114e-03, 2.6161e-03,
        2.1311e-03, 1.7360e-03, 1.4142e-03, 1.1520e-03, 9.3847e-04, 7.6450e-04,
        6.2277e-04, 5.0732e-04, 4.1327e-04, 3.3666e-04, 2.7425e-04, 2.2341e-04,
        1.8199e-04, 1.4825e-04, 1.2077e-04, 9.8381e-05, 8.0143e-05, 6.5286e-05,
        5.3183e-05, 4.3324e-05, 3.5292e-05, 2.8750e-05, 2.3420e-05, 1.9078e-05,
        1.5542e-05, 1.2660e-05, 1.0313e-05, 8.4015e-06, 6.8440e-06, 5.5752e-06,
        4.5417e-06, 3.6997e-06, 3.0139e-06, 2.4551e-06])
</code></pre>
<ol start="3">
<li>将频率转换为复数形式：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">freqs_for_each_token = torch.outer(torch.arange(<span class="hljs-number">17</span>), freqs)<br>freqs_cis = torch.polar(torch.ones_like(freqs_for_each_token), freqs_for_each_token)<br>freqs_cis.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64])
</code></pre>
<ol start="4">
<li>绘制第 3 个 token 位置对应的前 17 个频率分量的复数表示：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">value = freqs_cis[<span class="hljs-number">3</span>]<br>plt.figure()<br><span class="hljs-keyword">for</span> i, element <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(value[:<span class="hljs-number">17</span>]):<br>    plt.plot([<span class="hljs-number">0</span>, element.real], [<span class="hljs-number">0</span>, element.imag], color=<span class="hljs-string">&#x27;blue&#x27;</span>, linewidth=<span class="hljs-number">1</span>, label=<span class="hljs-string">f&quot;Index: <span class="hljs-subst">&#123;i&#125;</span>&quot;</span>)<br>    plt.annotate(<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;i&#125;</span>&quot;</span>, xy=(element.real, element.imag), color=<span class="hljs-string">&#x27;red&#x27;</span>)<br>plt.xlabel(<span class="hljs-string">&#x27;Real&#x27;</span>)<br>plt.ylabel(<span class="hljs-string">&#x27;Imaginary&#x27;</span>)<br>plt.title(<span class="hljs-string">&#x27;Plot of one row of freqs_cis&#x27;</span>)<br>plt.show()<br></code></pre></td></tr></table></figure>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/4.png" srcset="/img/loading.gif" lazyload class="">
<h3 id="4-1-5-query-rotated"><a href="#4-1-5-query-rotated" class="headerlink" title="4.1.5 query_rotated"></a>4.1.5 query_rotated</h3><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/5.png" srcset="/img/loading.gif" lazyload class="">

<ol>
<li>针对每个 token，将 128 个长度的 query 分为 64 对：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token_split_into_pairs = q_per_token.<span class="hljs-built_in">float</span>().view(q_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>q_per_token_split_into_pairs.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64, 2])
</code></pre>
<ol start="2">
<li>将 query 对转换为 query 复数：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>q_per_token_as_complex_numbers.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64])
</code></pre>
<ol start="3">
<li>进行点积以根据位置旋转 query 复数，每一个都旋转 $m*\theta$，$m$ 是旋转 query 的 token 的位置：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token_as_complex_numbers_rotated = q_per_token_as_complex_numbers * freqs_cis<br>q_per_token_as_complex_numbers_rotated.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64])
</code></pre>
<ol start="4">
<li>将旋转的 query 复数看作实数来返回旋转的 query 对：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers_rotated)<br>q_per_token_split_into_pairs_rotated.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64, 2])
</code></pre>
<ol start="5">
<li>将旋转的 query 对变成旋转的 query：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>q_per_token_rotated.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h3 id="4-1-6-key-rotated"><a href="#4-1-6-key-rotated" class="headerlink" title="4.1.6 key_rotated"></a>4.1.6 key_rotated</h3><ol>
<li>针对每个 token，将 128 个长度的 key 分为 64 对：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_per_token_split_into_pairs = k_per_token.<span class="hljs-built_in">float</span>().view(k_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>k_per_token_split_into_pairs.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64, 2])
</code></pre>
<ol start="2">
<li>将 key 对转换为 key 复数：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>k_per_token_as_complex_numbers.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64])
</code></pre>
<ol start="3">
<li>进行点积以根据位置旋转 key 复数，每一个都旋转 $m*\theta$，$m$ 是旋转 key 的 token 的位置。将旋转的 key 复数看作实数来返回旋转的 key 对：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>k_per_token_split_into_pairs_rotated.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 64, 2])
</code></pre>
<ol start="4">
<li>将旋转的 key 对变成旋转的 key：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>k_per_token_rotated.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h3 id="4-1-7-qk-score"><a href="#4-1-7-qk-score" class="headerlink" title="4.1.7 qk_score"></a>4.1.7 qk_score</h3><ol>
<li>将 query 和 key 相乘得到每个 token 相互映射的得分，表示每个 token 的 query 与每个 token 的 key 之间的相关度：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(head_dim)**<span class="hljs-number">0.5</span><br>qk_per_token.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 17])
</code></pre>
<ol start="2">
<li>绘制 qk_score 矩阵的热力图：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">display_qk_heatmap</span>(<span class="hljs-params">qk_per_token</span>):<br>    _, ax = plt.subplots()<br>    im = ax.imshow(qk_per_token.to(<span class="hljs-built_in">float</span>).detach(), cmap=<span class="hljs-string">&#x27;viridis&#x27;</span>)<br>    ax.set_xticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(prompt_split_as_tokens)))<br>    ax.set_yticks(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(prompt_split_as_tokens)))<br>    ax.set_xticklabels(prompt_split_as_tokens)<br>    ax.set_yticklabels(prompt_split_as_tokens)<br>    ax.figure.colorbar(im, ax=ax)<br>    <br>display_qk_heatmap(qk_per_token)<br></code></pre></td></tr></table></figure>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/6.png" srcset="/img/loading.gif" lazyload class="">

<ol start="3">
<li>在 Llama3 的训练过程中，只学习使用过去的 token 来预测 token，因此将未来的 token 屏蔽为零。定义屏蔽矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">mask = torch.full((<span class="hljs-built_in">len</span>(tokens), <span class="hljs-built_in">len</span>(tokens)), <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>), device=tokens.device)<br>mask = torch.triu(mask, diagonal=<span class="hljs-number">1</span>)<br>mask<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf, -inf],
        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., -inf],
    [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])
</code></pre>
<ol start="4">
<li>绘制屏蔽后 qk_score 矩阵的热力图：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">qk_per_token_after_masking = qk_per_token + mask<br>display_qk_heatmap(qk_per_token_after_masking)<br></code></pre></td></tr></table></figure>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/7.png" srcset="/img/loading.gif" lazyload class="">

<ol start="5">
<li>绘制 Softmax 后 qk_score 矩阵的热力图：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=<span class="hljs-number">1</span>).to(torch.bfloat16)<br>display_qk_heatmap(qk_per_token_after_masking_after_softmax)<br></code></pre></td></tr></table></figure>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/8.png" srcset="/img/loading.gif" lazyload class="">

<h3 id="4-1-8-qkv-score"><a href="#4-1-8-qkv-score" class="headerlink" title="4.1.8 qkv_score"></a>4.1.8 qkv_score</h3><p>qk_score 和每个 token 的 value 相乘后得到 qkv_score 矩阵：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>qkv_attention.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 128])
</code></pre>
<h2 id="4-2-多头注意力"><a href="#4-2-多头注意力" class="headerlink" title="4.2 多头注意力"></a>4.2 多头注意力</h2><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/9.png" srcset="/img/loading.gif" lazyload class="">

<ol>
<li>执行循环，来计算第一层中剩余 31 个注意力头的 qkv_score：</li>
</ol>
<blockquote>
<p>现实中每一层中的所有注意力头是并行计算的。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python">qkv_attention_store = []<br><br><span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_heads):<br>    q_layer0_head = q_layer0[head]<br>    k_layer0_head = k_layer0[head//<span class="hljs-number">4</span>]<br>    v_layer0_head = v_layer0[head//<span class="hljs-number">4</span>]<br>    q_per_token = torch.matmul(token_embeddings, q_layer0_head.T)<br>    k_per_token = torch.matmul(token_embeddings, k_layer0_head.T)<br>    v_per_token = torch.matmul(token_embeddings, v_layer0_head.T)<br><br>    q_per_token_split_into_pairs = q_per_token.<span class="hljs-built_in">float</span>().view(q_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>    q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis[:<span class="hljs-built_in">len</span>(tokens)])<br>    q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br><br>    k_per_token_split_into_pairs = k_per_token.<span class="hljs-built_in">float</span>().view(k_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>    k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis[:<span class="hljs-built_in">len</span>(tokens)])<br>    k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br><br>    qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(<span class="hljs-number">128</span>)**<span class="hljs-number">0.5</span><br>    mask = torch.full((<span class="hljs-built_in">len</span>(tokens), <span class="hljs-built_in">len</span>(tokens)), <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>), device=tokens.device)<br>    mask = torch.triu(mask, diagonal=<span class="hljs-number">1</span>)<br>    qk_per_token_after_masking = qk_per_token + mask<br>    qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=<span class="hljs-number">1</span>).to(torch.bfloat16)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>    qkv_attention_store.append(qkv_attention)<br><br><span class="hljs-built_in">len</span>(qkv_attention_store)<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">32
</code></pre>
<ol start="2">
<li>现在第一层上的所有 32 个头都有了 qkv_score，合并为一个大矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-<span class="hljs-number">1</span>)<br>stacked_qkv_attention.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<h2 id="4-3-output"><a href="#4-3-output" class="headerlink" title="4.3 output"></a>4.3 output</h2><ol>
<li>查看第一层的 output 权重矩阵：</li>
</ol>
<blockquote>
<p>key 和 value 的维度被减小是为了减少计算复杂度和内存消耗，而保持 query 和 output 的较高维度是为了保留更多的信息。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">w_layer0 = model[<span class="hljs-string">&quot;layers.0.attention.wo.weight&quot;</span>]<br>w_layer0.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([4096, 4096])
</code></pre>
<ol start="2">
<li>将合并的大 qkv_score 矩阵和第一层的 output 权重矩阵进行矩阵乘法：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">embedding_delta = torch.matmul(stacked_qkv_attention, w_layer0.T)<br>embedding_delta.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<h1 id="5-第一层-Layer-的剩余流程"><a href="#5-第一层-Layer-的剩余流程" class="headerlink" title="5 第一层 Layer 的剩余流程"></a>5 第一层 Layer 的剩余流程</h1><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/10.png" srcset="/img/loading.gif" lazyload class="">
<ol>
<li>进行残差连接：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">embedding_after_edit = token_embeddings_unnormalized + embedding_delta<br>embedding_after_edit.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<ol start="2">
<li>进行归一化：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[<span class="hljs-string">&quot;layers.0.ffn_norm.weight&quot;</span>])<br>embedding_after_edit_normalized.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<ol start="3">
<li>进行 MLP：</li>
</ol>
<blockquote>
<p>MLP 是一种特殊的 FFN，比传统 FFN 的维度小，利用率高。它引入了门控技术，gate_proj 为门控投影，up_proj 为升维投影，down 为降维投影，SiLU 用于门控过滤信息。</p>
</blockquote>
<img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/11.png" srcset="/img/loading.gif" lazyload class="">

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">w1 = model[<span class="hljs-string">&quot;layers.0.feed_forward.w1.weight&quot;</span>]<br>w2 = model[<span class="hljs-string">&quot;layers.0.feed_forward.w2.weight&quot;</span>]<br>w3 = model[<span class="hljs-string">&quot;layers.0.feed_forward.w3.weight&quot;</span>]<br>output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>output_after_feedforward.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<ol start="4">
<li>进行残差连接：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">layer_0_embedding = embedding_after_edit+output_after_feedforward<br>layer_0_embedding.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<h1 id="6-升维和降维"><a href="#6-升维和降维" class="headerlink" title="6 升维和降维"></a>6 升维和降维</h1><p>升维通常是为了增加模型的容量，使其能够捕捉更复杂的特征和模式。当输入数据被映射到一个更高维度的空间时，不同的特征组合可以被模型更容易地区分。这在处理非线性问题时尤其有用，因为它可以帮助模型学习到更复杂的决策边界 。</p>
<p>降维则是为了减少模型的复杂性和过拟合的风险。通过减少特征空间的维度，模型可以被迫学习更加精炼和泛化的特征表示。此外，降维可以作为一种正则化手段，有助于提高模型的泛化能力。在某些情况下，降维还可以减少计算成本和提高模型的运行效率 。</p>
<p>在实际应用中，升维后再降维的策略可以被视为一种特征提取和变换的过程。在这个过程中，模型首先通过增加维度来探索数据的内在结构，然后通过降维来提取最有用的特征和模式。这种方法可以帮助模型在保持足够复杂性的同时，避免过度拟合训练数据 。</p>
<h1 id="7-每层-layer"><a href="#7-每层-layer" class="headerlink" title="7 每层 layer"></a>7 每层 layer</h1><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/12.png" srcset="/img/loading.gif" lazyload class="">
<p>现在终于在第一层之后为每个 token 提供了新的 token embedding，之后的 31 层也是一样的处理过程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python">final_embedding = token_embeddings_unnormalized<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_layers):<br>    qkv_attention_store = []<br>    layer_embedding_norm = rms_norm(final_embedding, model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention_norm.weight&quot;</span>])<br>    q_layer = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention.wq.weight&quot;</span>]<br>    q_layer = q_layer.view(n_heads, q_layer.shape[<span class="hljs-number">0</span>] // n_heads, dim)<br>    k_layer = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention.wk.weight&quot;</span>]<br>    k_layer = k_layer.view(n_kv_heads, k_layer.shape[<span class="hljs-number">0</span>] // n_kv_heads, dim)<br>    v_layer = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention.wv.weight&quot;</span>]<br>    v_layer = v_layer.view(n_kv_heads, v_layer.shape[<span class="hljs-number">0</span>] // n_kv_heads, dim)<br>    w_layer = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention.wo.weight&quot;</span>]<br>    <span class="hljs-keyword">for</span> head <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(n_heads):<br>        q_layer_head = q_layer[head]<br>        k_layer_head = k_layer[head//<span class="hljs-number">4</span>]<br>        v_layer_head = v_layer[head//<span class="hljs-number">4</span>]<br>        q_per_token = torch.matmul(layer_embedding_norm, q_layer_head.T)<br>        k_per_token = torch.matmul(layer_embedding_norm, k_layer_head.T)<br>        v_per_token = torch.matmul(layer_embedding_norm, v_layer_head.T)<br>        q_per_token_split_into_pairs = q_per_token.<span class="hljs-built_in">float</span>().view(q_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        q_per_token_as_complex_numbers = torch.view_as_complex(q_per_token_split_into_pairs)<br>        q_per_token_split_into_pairs_rotated = torch.view_as_real(q_per_token_as_complex_numbers * freqs_cis)<br>        q_per_token_rotated = q_per_token_split_into_pairs_rotated.view(q_per_token.shape)<br>        k_per_token_split_into_pairs = k_per_token.<span class="hljs-built_in">float</span>().view(k_per_token.shape[<span class="hljs-number">0</span>], -<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>        k_per_token_as_complex_numbers = torch.view_as_complex(k_per_token_split_into_pairs)<br>        k_per_token_split_into_pairs_rotated = torch.view_as_real(k_per_token_as_complex_numbers * freqs_cis)<br>        k_per_token_rotated = k_per_token_split_into_pairs_rotated.view(k_per_token.shape)<br>        qk_per_token = torch.matmul(q_per_token_rotated, k_per_token_rotated.T)/(<span class="hljs-number">128</span>)**<span class="hljs-number">0.5</span><br>        mask = torch.full((<span class="hljs-built_in">len</span>(token_embeddings_unnormalized), <span class="hljs-built_in">len</span>(token_embeddings_unnormalized)), <span class="hljs-built_in">float</span>(<span class="hljs-string">&quot;-inf&quot;</span>))<br>        mask = torch.triu(mask, diagonal=<span class="hljs-number">1</span>)<br>        qk_per_token_after_masking = qk_per_token + mask<br>        qk_per_token_after_masking_after_softmax = torch.nn.functional.softmax(qk_per_token_after_masking, dim=<span class="hljs-number">1</span>).to(torch.bfloat16)<br>        qkv_attention = torch.matmul(qk_per_token_after_masking_after_softmax, v_per_token)<br>        qkv_attention_store.append(qkv_attention)<br><br>    stacked_qkv_attention = torch.cat(qkv_attention_store, dim=-<span class="hljs-number">1</span>)<br>    w_layer = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.attention.wo.weight&quot;</span>]<br>    embedding_delta = torch.matmul(stacked_qkv_attention, w_layer.T)<br>    embedding_after_edit = final_embedding + embedding_delta<br>    embedding_after_edit_normalized = rms_norm(embedding_after_edit, model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.ffn_norm.weight&quot;</span>])<br>    w1 = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.feed_forward.w1.weight&quot;</span>]<br>    w2 = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.feed_forward.w2.weight&quot;</span>]<br>    w3 = model[<span class="hljs-string">f&quot;layers.<span class="hljs-subst">&#123;layer&#125;</span>.feed_forward.w3.weight&quot;</span>]<br>    output_after_feedforward = torch.matmul(torch.functional.F.silu(torch.matmul(embedding_after_edit_normalized, w1.T)) * torch.matmul(embedding_after_edit_normalized, w3.T), w2.T)<br>    final_embedding = embedding_after_edit+output_after_feedforward<br>    <br>    <span class="hljs-comment"># 查看每层输入和输出 MLP 的 embedding 中映射的 logits 中概率最大的 token</span><br>    before_mlp_embedding = embedding_after_edit_normalized<br>    after_mlp_embedding = rms_norm(final_embedding, model[<span class="hljs-string">&quot;norm.weight&quot;</span>])<br>    before_mlp_logits = torch.matmul(before_mlp_embedding[-<span class="hljs-number">1</span>], model[<span class="hljs-string">&quot;output.weight&quot;</span>].T)<br>    after_mlp_logits = torch.matmul(after_mlp_embedding[-<span class="hljs-number">1</span>], model[<span class="hljs-string">&quot;output.weight&quot;</span>].T)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f&quot;输入 MLP：<span class="hljs-subst">&#123;tokenizer.decode([torch.argmax(before_mlp_logits, dim=-<span class="hljs-number">1</span>).item()])&#125;</span>, 输出 MLP：<span class="hljs-subst">&#123;tokenizer.decode([torch.argmax(after_mlp_logits, dim=-<span class="hljs-number">1</span>).item()])&#125;</span>&quot;</span>)<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">输入 MLP：ival, 输出 MLP：Disposition
输入 MLP：ौल, 输出 MLP：.updateDynamic
输入 MLP：opsy, 输出 MLP： Oaks
输入 MLP： Oaks, 输出 MLP：.stamp
输入 MLP：_stamp, 输出 MLP：RYPTO
输入 MLP：anker, 输出 MLP：ズ
输入 MLP：ズ, 输出 MLP：лишком
输入 MLP：BERS, 输出 MLP： nông
输入 MLP：ンチ, 输出 MLP：ilio
输入 MLP： Sez, 输出 MLP：tempts
输入 MLP：ilio, 输出 MLP：HAV
输入 MLP：HAV, 输出 MLP：ustum
输入 MLP： nebu, 输出 MLP：CRET
输入 MLP： Roose, 输出 MLP：\Dependency
输入 MLP：�, 输出 MLP：#af
输入 MLP：wang, 输出 MLP：iteDatabase
输入 MLP：SEX, 输出 MLP：&#39;gc
输入 MLP：STRUCTIONS, 输出 MLP：ęk
输入 MLP：ęk, 输出 MLP：&#39;gc
输入 MLP： answers, 输出 MLP： answer
输入 MLP： answer, 输出 MLP：рд
输入 MLP：рд, 输出 MLP：answered
输入 MLP：answered, 输出 MLP：answered
输入 MLP：answered, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
输入 MLP：42, 输出 MLP：42
</code></pre>
<h1 id="8-LogitLens"><a href="#8-LogitLens" class="headerlink" title="8 LogitLens"></a>8 LogitLens</h1><img src="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E6%A8%A1%E5%9E%8B%E5%A4%8D%E7%8E%B0%E3%80%91%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0-llama3/13.png" srcset="/img/loading.gif" lazyload class="">

<ol>
<li>经过 32 层 Layers 后，得到了最终的 token embedding，对其进行归一化：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">final_embedding = rms_norm(final_embedding, model[<span class="hljs-string">&quot;norm.weight&quot;</span>])<br>final_embedding.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([17, 4096])
</code></pre>
<ol start="2">
<li>查看最后一个线性层的权重矩阵：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">model[<span class="hljs-string">&quot;output.weight&quot;</span>].shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([128256, 4096])
</code></pre>
<ol start="3">
<li>得到下一个预测的 token 的概率分布（通常还要对概率分布进行 Softmax）：</li>
</ol>
<blockquote>
<p>模型中最后一个线性层的输出称为 logits，表示未缩放的“概率”，但总和不为1，因此需要 Softmax。只有最后一个 token embedding 用于预测</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">logits = torch.matmul(final_embedding[-<span class="hljs-number">1</span>], model[<span class="hljs-string">&quot;output.weight&quot;</span>].T)<br>logits.shape<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">torch.Size([128256])
</code></pre>
<ol start="4">
<li>取其概率最高的 token 作为预测结果：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">next_token = torch.argmax(logits, dim=-<span class="hljs-number">1</span>)<br>next_token<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">tensor(2983)
</code></pre>
<ol start="5">
<li>对预测的 token 解码：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">tokenizer.decode([next_token.item()])<br></code></pre></td></tr></table></figure>
<br/>

<pre><code class="hljs">&#39;42&#39;
</code></pre>
<h1 id="9-采样策略"><a href="#9-采样策略" class="headerlink" title="9 采样策略"></a>9 采样策略</h1><ol>
<li>Greedy Search：每一步自回归都选择概率最高的 token。</li>
<li>Beam Search：保留固定束宽的候选序列，最终选择整体概率最高的序列。</li>
<li>Top-K：仅从概率最高的 K 个 token 中采样。</li>
<li>Top-P：动态选择累积概率超过 P 的最小 token 集合。</li>
<li>Random Sampling：按照概率分布随机采样。</li>
<li>Temperature：温度越高，概率分布越平缓，多样性越高；温度越低，概率分布越陡峭，风格越鲜明。</li>
</ol>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/" class="category-chain-item">代码复现</a>
  
  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/LLM/" class="print-no-link">#LLM</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>【模型复现】从零实现 Llama3</div>
      <div>http://xuan-van.github.io/代码复现/【模型复现】从零实现-llama3/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>文晋</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2025年4月1日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-cc-by"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - 非商业性使用">
                    <i class="iconfont icon-cc-nc"></i>
                  </span>
                </a>
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - 相同方式共享">
                    <i class="iconfont icon-cc-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/%E9%9A%8F%E7%AC%94/%E3%80%90%E6%B8%B8%E6%88%8F%E5%9B%9E%E5%BF%86%E5%BD%95%E3%80%91%E8%B0%88%E8%B0%88%E9%82%A3%E4%BA%9B%E5%B9%B4%E4%BB%A4%E6%88%91%E5%8D%B0%E8%B1%A1%E6%B7%B1%E5%88%BB%E7%9A%84%E6%B8%B8%E6%88%8F/" title="【游戏回忆录】谈谈那些年令我印象深刻的游戏">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">【游戏回忆录】谈谈那些年令我印象深刻的游戏</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/%E3%80%90%E8%AE%BA%E6%96%87%E5%A4%8D%E7%8E%B0%E3%80%91hipporag-hipporag2/" title="【论文复现】HippoRAG &amp; HippoRAG2">
                        <span class="hidden-mobile">【论文复现】HippoRAG &amp; HippoRAG2</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.utils.listenDOMLoaded(function() {
      Fluid.events.registerRefreshCallback(function() {
        if ('mermaid' in window) {
          mermaid.init();
        }
      });
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>





  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/5.0.0/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>




  
<script src="/background/background.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
<!-- hexo injector body_end start --><script src="/background/background.js"></script><!-- hexo injector body_end end --></body>
</html>
